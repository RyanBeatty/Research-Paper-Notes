\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Research Paper Notes}
\author{You}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section{Apache}

\subsection{Spark: Cluster Computing with Working Sets}

\begin{itemize}

\item Introduction
\begin{itemize}
\item Spark is a cluster computing framework that targets applications where a working set of data is reused iteratively. Iterative jobs (such as some machine learning algorithms) and Interactive analytics (such as querying large amounts of data in a database) are prime examples of such applications

\item Spark's main abstraction is that of Resilient Distributed Datasets (RDDs) which are a read only collection of data that is partitioned across multiple machines with fault tolerance
\end{itemize}

\item Programming Model
\begin{itemize}

\item Programmers write a driver application which contains high level instructions for performing a parallel computation

\item The two main abstractions for parallel programming are Resilient Distributed Datasets and Parallel Operations

\item RDDs
\begin{itemize}
\item RDDs are a collection of data objects distributed over a cluster of computers such that any partition can be rebuilt if a node fails

\item RDD handles contain enough information to rebuild the elements of the set from data in reliable storage. This means that elements of an RDD need not be held in physical storage and can be rebuilt if a node fails

\item RDDs can be built in 4 ways:
\begin{enumerate}
\item By parallelizing a collection (e.g. an array) in the driver program into slices to be sent to worker nodes

\item By transforming an existing RDD using operations such as flatMap, map, and reduce (Note: this suggests that RDDs form a Monad)

\item By changing the persistence of an RDD. By default RDDs are lazy and are discarded after use. An RDD can be reused either by enabling cache hinting (e.g. cache the RDD if possible) or saving the RDD after operations have been performed on it
\end{enumerate}

\item Spark supports the reduce, collect, and foreach parallel operations on RDDs. Reduce combines elements of an RDD using an associative function for collection at the driver program. Collect sends elements of the RDD to the driver program (this allows easy updates of collections such as arrays). Foreach passes each element of an RDD through a function which has side effects (e.g. copying the data to another filesystem, etc). 

\item Spark supports two notions of shared state: Broadcast variables and accumulators

\item Broadcast variables are read only pieces of data that each worker node has access to. This allows programmers to only copy this data to each worker once instead of having to bundle it in every closure it sends to the workers (Note: this seems to me like a case of the Reader Monad)

\item Accumulators are values that only workers can add to using an associative function and which only the driver can read. This can be used to implement counters. Note: accumulators are Monoids. 

\end{itemize}

\item Implementation
\begin{itemize}
\item Spark is built on top of Apache Mesos which is a framework for cluster computing

\item RDDs are stored as a chain of objects forming a lineage. Each dataset points to its parent dataset and stores enough information to know how the parent was transormed.

\item RDDs internally all implement the same interface: getPartitions() which returns a list of partition ids, getIterator(partition) which iterates over a partition, and getPrefferedLocations(partition) which is used for task scheduling to achieve data locality.

\item Spark creates tasks for each partition of a parallel operation and attempts to send these tasks to preferred worker nodes using delay scheduling. Once a worker receives a partition, it calls getIterator() to start iterating over it.

\item Shipping tasks to workers involves shipping both the closures used to defined the RDD and the ones used to operate on it. Spark uses Java's serialization to ship closures

\item Broadcast variables are initially stored in a shared file system and then cached by worker nodes on demand. The serialization format for a broadcast variable is a path to the file storing its data

\item Accumulators are created for each thread running a task on a worker and "zeroed-out" after the task completes. Workers send the final value of task accumulators to the driver program for collection. Serialized accumulators contain a unique id and the "zero value" for the accumulator
\end{itemize}

\item Related Work
\begin{itemize}
\item RDDs are an abstraciton of Distributed Shared Memory (DSM)

\item RDD and DSM differ in two ways: DSM normally uses checkpointing to recover from node failure while RDD uses cheaper lineage information to rebuild lost partitions, and RDDs push computation to the data while DSM operates on a global address space

\item The key component of Spark is allowing data to persist across iterations of a distributed job

\item Lineage on datasets is a well studied area
\end{itemize}

\end{itemize}

\end{itemize}

\section{Amazon}

\subsection{Dynamo: Amazon's Highly Available Key-value Store}

\begin{itemize}
\item System Assumptions and Requirements
\begin{itemize}
\item Query Model: items are binary blobs that are uniquely identified by a primary key. No complex operations. No relational schema

\item  ACID properties: data stores with ACID guarantees provide poor availability when scaling. Dynamo is good for applications with a looser consistency requirement. No isolation guarantees, so only single key updates

\item Dynamo is assumed to be run in a non-hostile environment at Amazon, so there are no security requirements. 
\end{itemize}

\item Design Considerations
\begin{itemize}
\item Synchronous replication of data will give strong consistency, but poor availability in the presence of certain failures

\item Availability can be increased for systems prone to network or server failures by using eventually consistent replication techniques. Writes are propagated to replicas asynchronously and in the background. Conflicting updates must be resolved. When to resolve updates and who resolves them?

\item Update conflicts are resolved during reads so that Dynamo can be an "always writeable" store. This is in contrast to traditional data stores which handle resolution during writes in order to keep reads simple (this can lead to writes being rejected if they cannot reach or are not accepted by the replicas).

\item Either the data store or the application can be the entity that resolves write conflicts. Application developers have better knowledge of the schema and might be able to perform better resolutions. Having the data store resolve conflicts simplifies the application logic. A typical conflict resolution strategy at the data store level is "last write wins".

\item Dynamo should be incrementally scalable

\item Dynamo instances should be symmetric/homogeneous (i.e. no distinguished nodes with extra responsibilities)

\item Dynamo should be decentralized and use peer-to-peer techniques over centralized control. Amazon believes decentralization results in higher availability

\item Dynamo should be able to run on heterogeneous hardware so that portions of a Dynamo cluster can be vertically scaled without having to scale all the nodes at once.
\end{itemize}

\item What Dynamo is built for
\begin{enumerate}
\item Targets applications that need an "always writeable" data store with no rejected updates.

\item Built for a network where all nodes are trusted

\item Targets applications that do not need hierarchical namespaces or relational schema

\item Built for latency sensitive applications. Multi hop routing for Distributed Hash Tables (e.g. Chord) incurs more latency and variability in terms of SLA. Dyanmo is a zero-hop DHT 
\end{enumerate}

\item System Architecture
\begin{itemize}
\item Techniques used by Dynamo
\begin{itemize}
\item Consistent Hashing solves the problem of Partitioning with the advantage of Incremental Scalability

\item Vector clocks with reconciliation during reads are used to allow for high availability of writes with the bonus that Version size is decoupled from update rates

\item Sloppy Quorum and hinted handoffs solves handling temporary failures with the bonus of providing high availability and durability guarantees when some of the replicas are not available

\item Anti-entropy using Merkle Trees solves recovering from permanent failures with the bonus of synchronizing divergent replicas in the background

\item Gossip based membership protocol and failure detection solves membership and failure detection with the bonus of keeping membership information decentralized
\end{itemize}

\item System Interface
\begin{itemize}
\item Exposes two operations $get()$ and $put()$

\item $get(key)$ returns the object associated with $key$ or a list of objects with conflicting versions along with a $context$.

\item $put(key, context, object)$ stores the replicas of $object$ based on $key$ and writes the replicas to disk. 

\item $context$ encodes system metadata (such as the version of the object) about the object that is hidden from the caller. The $context$ is stored along with the $object$ in order for the system to do validation.

\item Both the $object$ and $key$ are treated as an array of bytes. Dynamo computes an MD5 hash of the $key$ to get a 128-bit identifier to determine the storage nodes responsible for serving the $key$. 
\end{itemize}

\item Partitioning Algorithm
\begin{itemize}
\item In order to scale incrementally, Dynamo needs a way to dynamically partition the data over the set of nodes in the system. Therefore, Dynamo relies on consistent hashing to distribute load across multiple Dynamo instances.

\item Consistent hashing works by treating the output of the hash function as a fixed circular array or ring (the largest hash value wraps around to the smallest hash value). Each node in the system is assigned a random position in the ring. The keys assigned to a node are all of the keys that lie between its position in the ring and its predecessor's position. The main advantage of consistent hashing is that departure or arrival of nodes in the system only effects the immediate neighbours of the node in the ring.

\item Basic consistent hashing algorithms can lead to non uniform data and load distribution since each node is assigned a random spot in the ring. Also it doesn't take into account the heterogeneous hardware that each node is running on.

\item Dynamo uses a modified consistent hashing algorithm where each node in the system gets mapped to multiple positions in the key space. Nodes can be responsible for many of these "virtual nodes" (single positions in the key space). When a new node joins the system it is assigned many multiple positions in the key space (also called "tokens").

\item Virtual nodes allow even load shedding across the rest of the nodes in the system if a single node leaves. When a node joins the system, it takes on roughly the same amount of load from each node. The number of virtual nodes a host is responsible for can be tuned to the amount of capacity the host has based on the hardware it is running on
\end{itemize}

\item Replication
\begin{itemize}
\item Dynamo provides high availability and durability by replicating data across multiple nodes.

\item Each node stores all keys that fall within its range locally and replicates them across the next $N-1$ successor nodes (where $N$ is configurable per node) so that all data is replicated across $N$ nodes. This implies that each node is responsible for storing keys in the range of itself and its $Nth$ predecessor.

\item The $preference list$ contains the nodes which are responsible for storing a particular key.

\item Each node contains enough information so that it can determine the preference list for any key

\item The preference list has more than $N$ nodes to account for failure

\item The preference list contains distinct nodes. When building the preference list, virtual nodes that belong to nodes that are already in the list are skipped. 
\end{itemize}

\item Data Versioning
\begin{itemize}
\item Dynamo allows write to propagate asynchronously, providing eventual consistency

\item Dynamo treats an updated object as a new, immutable version of the object and allows for multiple versions of an object to be present within the system.

\item Newer versions of an object normally subsume older versions; however, this might not be the case in the presence  of failures and concurrent updates. In this case where version branching occurs, the application must do conflict resolution on a subsequent read to collapse these versioned branches

\item Dynamo uses vector clocks in order to determine which versions of an object "happened before" other versions. A vector clock is a list of (node, counter) pairs and each version of an object has its own vector clock. If all of the counters in a vector clock are less than or equal to all of the counters in vector clock of a second version, then the first is an ancestor of the second and can be forgotten, else the two versions are in conflict

\item Updating an item in Dynamo is done by passing it a context (which contains the vector clock info) which was obtained by an earlier read operation. If a read request for an object detects that there are multiple versions of the object in conflict, Dynamo will return all of the objects and put all of the vector clock information for each version in the context returned. A subsequent write with this context is assumed by Dynamo to have reconciled all of the different versions, so it will collapse the multiple versions

\item Theoretically the size of the vector clock could grow to a large size if many nodes are coordinating writes for an object. In practice, this shouldn't happen since only the $N$ nodes in the preference list should be handling writes for the object; however, network partitions might make this not be the case

\item Dynamo truncates the vector clock to a fixed size. It adds to each (node, count) pair a timestamp of when the node last serviced the object. If the clock is full it evicts the entry with the oldest timestamp. This can lead to situations where causal dependency between object versions cannot be accurately reconstructed, but Amazon has neither encountered or explored this
\end{itemize}

\item Execution of get() and put() without failures
\begin{itemize}
\item Operations are invoked over HTTP

\item Clients can either send requests through a load balancer to route to a node or link in a partition-aware library that routes requests directly to the right coordinator node

\item A loadbalancer might send a request to a node that is not in the $N$ number of nodes for the keys preference list. This node will then send the request to the first node in the preference list for the key

\item Read and write operations use the first $N$ healthy nodes in the preference list for a key, skipping unhealthy nodes. When there are node failures, lower ranked nodes (those not in the first $N$ nodes of the preference list) will be employed

\item Dynamo has two configurable values ($R$ and $W$) which it will use in its quorum-like consistency algorithm to maintain consistency between replicas for a key. $R$ and $W$ are the minimum number of nodes that must participate in a successful read or write operation respectively. Setting $R + W > N$ yields a quorum-like model. However, since the latency of a ready (or write) operation is dominated by the slowest of the $R$ (or $W$), replicas, $R$ and $W$ are configured to be much smaller than $N$

\item For a put() request, the coordinator generates the new vector clock and version and stores the version locally. It then sends both to the $N$ highest reachable nodes and waits for at least $W-1$ successful responses before returning success.

\item For a get() request, the coordinator requests all existing versions of data for a key from the $N$ highest reachable nodes and then waits for $R$ (not $R-1$ I guess?) responses before returning success. If the coordinator gets multiple versions for the object, it returns all of the ones it deems causally unrelated (needing to be resolved)
\end{itemize}

\item Handling Failures: Hinted Handoff
\begin{itemize}
\item Dynamo uses a "sloppy quorum" approach to coordinate reads and writes in order to maintain high availability in the face of network failures and partitions

\item If a node in the top $N$ nodes of the preference list for a key is not healthy, another node (not in the top $N$) will be chosen

\item The node will receive metadata indicating which node was the intended recipient of the key originally.

\item The node will keep this keep in a separate local database and will periodically try to push keys in this database to their original receiver node once those nodes have come back online.

\item In order to survive data center failures, the preference list for a key should be configured such that it contains nodes spanning many different data centers
\end{itemize}

\item Handling Permanent Failures
\begin{itemize}
\item Hinted handoff works best if system membership churn is low and network failures are transient

\item To combat permanent failures, Dynamo uses an anti-entropy protocol along with Merkle trees to synchronize replicas

\item The Merkle tree builds a hash tree of all of the keys a virtual node is responsible for

\item Replicas compare Merkle trees with one another and fix inconsistencies
\end{itemize}

\item Membership and Failure Detection
\begin{itemize}
\item Dynamo uses an explicit mechanism to signal the permanent addition or removal of a node from the system (e.g. an operator use a CLI or browser to connect to a dynamo node and issue a membership request)

\item The node that services the membership change request writes the details of the request and a timestamp to persistent storage. This info is then propagated via a gossip protocol where nodes choose a peer at random to connect to every second and reconcile their membership tables

\item When a new node initially starts up, it chooses its own virtual nodes and persists this information. During membership reconciliation, nodes also reconcile key partition information. This allows nodes to have knowledge of which parts of the key space are owned by its peers in order to be able to route requests for those keys

\item Some nodes act as "seed" nodes which are known to all nodes. These nodes are typically well connected in the system and prevent logical partitions from being formed since all nodes will communicate with the seed nodes. Seed nodes are discovered via some configured or external method

\item Dynamo nodes only have a concept of local failure of other nodes (i.e. Node A might consider node B to be unhealthy if it is unresponsive even though node B is responsive to node C).

\item Dynamo doesn't need a globally consistent notion of failure because of the explicit node join and leave mechanism
\end{itemize}

\item Bootstrapping New Nodes
\begin{itemize}
\item New nodes that enter the system take over the keys of the tokens it has chosen in the key space

\item Older nodes that currently own these tokens detect the new node and offer to give the new node the keys it owns

\item Once the new node accepts the offer and receives the tokens, the older nodes can forget their tokens

\item Confirmation round ensures the new node doesn't receive any duplicate transfers for the same key range
\end{itemize}

\item Implementation
\begin{itemize}
\item 3 components: request coordination, membership and failure detection, and a local persistence engine

\item Request coordinator is built using an event-driven solution

\item Request handling is implemented as a state machine

\item When servicing a read, if a response from a replica node is received by the coordinator after it has sent a response to the client and the replica node's response indicates that the objects the coordinator returned were stale, the coordinator will do read repair and update the read replica nodes with the up to date version of the object

\item Write coordinators are typically chosen to be the node in the preference list that responded the quickest
\end{itemize}

\item Experiences and Lessons Learned
\begin{itemize}
\item Services either employ business logic or "last write wins" to handle conflict resolution

\item Configuring N, W, and R really effect availability, durability and performance. Most services use (3, 2, 2) for (N, R, W)

\item An optimization for writes is to have each node maintain an in-memory cache of objects. Writes are stored in the cache and a writer thread periodically flushes the cache to persistent storage. It should be noted this trades durability for performance. Coordinator nodes can also choose one replica to do a durable write such that the node will only store the write in its persistent storage, this helps mitigate total loss of writes during failures

\item Experience shows that divergent versions numbers are an extremely rare phenomenon and mostly occur when there are a large number of concurrent writes for the same object (normally initiated by an automated client)

\item Another approach to request routing is to have it all be on the client side. A client could download membership information from a Dynamo node every 10 seconds and send requests to the correct node and bypass a hop to the load balancer or to the correct coordinator node (in the case of writes). Pull model is used because it scales better with many clients

\item Nodes must be careful not to allow the scheduling of background tasks (hinted handoff, replica synchronization) to create resource contention that could slow down foreground tasks (put, get). A monitor task monitors the nodes resource utilization and reserves slices of time for the background tasks to run
\end{itemize}

\end{itemize}
\end{itemize}

\section{Apache}

\subsection{Zookeeper: Wait-free coordination for Internet-scale systems}

Zookeeper is a service for coordinating distributed processes

Zookeeper appears to be Chubby but without the $open$ or $close$ locking methods

\section{Blockchain}

\subsection{Tezos — a self-amending crypto-ledger White paper}

Proposes a new cryptocurrency where users can vote to change the blockchain policy at any point.

\begin{itemize}
\item A blockchain protocol can be broken down into 3 distinct protocols
\begin{itemize}
\item Network protocol: discovers blocks and broadcasts transactions
\item Transaction protocol: determines what makes transactions valid
\item Consensus protocol: forms consensus around a unique chain
\end{itemize}

\item A blockchain protocol is fundamentally a monadic implementation of concurrent mutations of a global state. This is achieved by defining “blocks” as operators (functions) acting on this global state. The free monoid of blocks acting on the genesis state forms a tree structure (set of all possible block combinations). A global, canonical, state is defined as the minimal leaf for a specified ordering.

\item Let $(S, \leq)$ be a totally ordered, countable set of possible states. Note: a totally ordered set is a set $X$ along with a binary relation $\leq$ such that

\begin{itemize}
\item Reflexive: $a \leq a$ for all $a \in X$
\item Antisymmetric: If $a \leq b$ and $b \leq a$ then $a = b$ for all $a,b \in X$
\item Transitive: If $a \leq b$ and $b \leq c$ then $a \leq c$ for all $a,b,c \in X$
\item Totality: $a \leq b$ or $b \leq a$ for all $a, b \in X$
\end{itemize}

Totally ordered sets form a category and can be understood as partially ordered sets with additional structure

\item Let $\emptyset \notin S$ be a special, invalid state

\item Let $B \subset S^{S \cup \{\emptyset\}}$ be the set of blocks (note that $S^{S \cup \emptyset}$ is the set of functions with domain $S \cup \emptyset$ and range $S$). The set of valid blocks in $B \cap S^S$

\item We extend the total order on $S$ with $\forall s \in S, \emptyset < s$. I believe this means $\emptyset$ is the bottom of $S$. This order determines which leaf in the block tree is the canonical one

\item Any blockchain protocol can be identified by $(S, \leq, \emptyset, B \subset S^{S \cup \{\emptyset\}})$

\item In Tezos blocks can not only act on the global state but also the protocol itself. The set of all possible protocols is defined recursively as $\mathcal{P}=\{(S, \leq, \emptyset, B \subset S^{(S \times \mathcal{P}) \cup \{\emptyset\}})\}$. The last element of the tuple is the formalisation that blocks can act on both state and the protocol itself.

\item The network shell
\begin{itemize}
\item knows how to build the block tree
\item Knows of 3 types of objects: transactions, blocks, and protocols. In Tezos, protocols are OCaml modules used to amend the existing protocol.
\item the hardest part of the newtwork layer is to protect the user against DOS attacks
\end{itemize}

\item Clocks
\begin{itemize}
\item Every block has a timestamp
\item Blocks with timestamps a reasonably amount of time in the future can be buffered, others are rejected
\item Must be able to deal with falsified timestamps and user clock drift
\end{itemize}

\item Chain Representation
\begin{itemize}
\item A raw block header contains: a hash to the previous block, a bytestring consisting of the block header (where transactions in this block are), a list of operations, and a timestamp in floating point notation.
\item State (or Context) is represented by an immutable, disk-based key-value store. Blocking on disk operations is avoided using asynchronous operations
\item A protocol is defined by several operations:
\begin{itemize}
\item Parsing operations to translate block headers and operations from byestrings into ADTs.
\item An apply operation which takes a state, a parsed block header, and a parsed list of operations, and returns the next state. This is the monadic abstraction talked about in the beginning of the paper: A blockchain protocol is a monadic implementation of concurrent mutations of a global state.
\item a score function that projects a State onto a list of bytes so that States can be compared (first by length then by lexicographic order). This comparison function is the total order ($\leq$) on the set of states described in the intro of the paper (ties are broken based on the hash of the last block). Converting State into a bytestring also allows us to states across different protocols.
\end{itemize}

\item Ammending the Protocol
\begin{itemize}
\item Two protocols: one for testing and the current protocol
\item Can swap out either protocol with a stakeholder vote
\item When a protocol is swapped out, the global State changes, and the new protocol takes effect when the next block is applied to the State.
\item A protocol is an OCaml module that is compiled on the fly and run inside a sandbox (no system calls can be made)
\item the protocol is used by the $apply$ function when computing a new $State$.
\item Many rules can be voted in to amend or change the protocol
\end{itemize}

\item Protocols expose an RPC mechanism so that GUIs can be made
\end{itemize}

\item Seed Protocol
\begin{itemize}
\item Tezos starts with a seed protocol
\item There is a finite amount of coin in the system
\item Relies on a combination of bonds and rewards to incentivize miners to secure the blockchain
\item Bonds are one year security deposits that are purchased by miners and forfeited if they break the rules
\item After a year, miners and endorsers receive a reward for their service as well as their security deposit back
\item Inactive addresses are not selected to propose blocks or vote for blocks in the consensus algorithm
\item Protocol amendments are proposed and voted over a period of time every 3 months. A quorum is needed for an amendment to pass.
\item During the first quarter of voting, participants propose the hash of a a tarball of an OCaml module representing the new protocol. Active users can choose to "approve" any number of the proposed protocols
\item During the second quarter, the amendment receiving the most approval from the first quarter is subject to a vote. Voters can vote yes, no, or abstain (which counts towards the quorum).
\item During the third quarter, if quorum was met and the amendment received a threshold of yes votes, then the protocol is adopted into the test network and the minimum quorum for future votes is adjusted based on the number of participants
\item During the fourth quarter, users vote again to promote the test protocol to replace the active protocol

\item Proof of Stake
\begin{itemize}
\item Blocks are mined by stakeholders and signed by signers. Mining and signing provide rewards to users but require a short term safety deposit that is forfeited in the event of malicious activity.
\item Groups of coins are grouped into coin rolls. Coin rolls are assigned priorities for signing the next block
\item Any stakeholder can sign a block. However, each stakeholder is subject to a random delay of how long they must wait before they can propose a block. This delay is determined by the priority assigned to any coin rolls they own (as I understand it)
\item Additionally 16 signers are chosen for each block. A blockchains weight is not its length but how many signatures it has.
\item Signers are incentived to sign quickly the best priority block it knows about in order to receive a reward

\end{itemize}

\item Smart Contracts
\begin{itemize}
\item Tezos uses stateful accounts
\item An account is simply a contract that has no executable code
\item contracts incur storage fees since they are stored in the blockchain (I guess?)
\item Contract code is only allowd to execute for a certain amount of "steps". Multiple contracts can be used to build larger programs. The threshold can of course be changed with a protocol amendment
\end{itemize}
\end{itemize}

\end{itemize}

\subsection{Secure High-Rate Transaction Processing in Bitcoin}
proposes the "Greediest Heaviest Observed Subtree" (GHOST) protocol extension to scale transaction confirmation for bitcoin to be able to handle a high enough transaction rate to meet global economic demands

\begin{itemize}
\item Nakamoto's original paper on bitcoin assumed that blocks could be transmitted through the overlay network much faster than they are created. The more transactions, the more frequently blocks must be created or the larger the block size must be and the longer it will take for new blocks to propagate throughout the network.

\item If the amount of transactions confirmed per second by the network does not rise, then transaction fees will raise and drive people to use other payment systems

\item In Nakamoto's original paper when multiple valid blocks are mined, a fork occurs and is not resolved until some chain of the fork becomes longer than the others. In the case of ties, a node simply adopts the block it learned of first as the main chain

\item The GHOST rule is an alternative rule for conflict resolution when forks occur. At each fork in the chain the heaviest subtree rooted at the fork is selected.

\item Ethereum is using a variant of the GHOST protocol

\item At a higher transaction confirmation rate, miners that are better connected may enjoy more rewards than there share of the hashing compute power in the network. Also selfish mining may become a viable strategy for weaker miners. To combat this, GHOST has an extension described in a different paper

\item Double spend attacks occur when a user pays a merchant some fee and then cafts a set of blocks (longer than the current honest main chain) without that transaction (or directing that transaction elsewhere) in order to replace the main chain. However this is not computationally feasible unless the attacker has more computational power than the entire honest network combined.

\item Model
\begin{itemize}
\item Let $V$ be the set of nodes in the bitcoin network and $E$ be the set of of edges connecting nodes. The network can then be modeled as a directed graph $G = (V, E)$.

\item Every node $v \in V$ has some fraction of the computational power of the network $p_v \ge 0$ such that the total computational power in the network can be defined as $\sum_{v\in V}p_v = 1$.

\item \textbf{Aside on Poisson Processes:} A Poisson process is a type of counting process that tries to model the occurrence of random events over time (the arrival of customers at a store or the number of earthquakes that will happen in a city). $\lambda$ is called the rate or intensity of the process and represents the rate at which the random event will happen per unit time.

\item The entire network creates blocks following a Poisson process with rate $\lambda$ with each node generating blocks individually following a Poisson process with rate $p_v \lambda$. The value of $\lambda = \frac{1}{600}$ (which I think is supposed to be 1 block per 600 seconds = 10 minutes) was chosen by Nakamoto in his original paper.

\item The time it takes to send a block across an edge $e \in E$ is denoted as $d_e$.

\item The block creation rate of the honest network is $\lambda_h = \lambda$. The attackers creation rate is denoted as $q\lambda_h > 0$ for some $0 < q < 1$. Furthermore, it is assumed that attackers build chains efficiently (meaning that an attackers' chain has minimal forks because it is able to coordinate its mining nodes to reach consensus near instantly when it creates a new block)

\item $time(B)$ is the absolute creation time for a block $B$. The structure of the tree at time $t$ is denoted by $tree(t)$ and by $subroot(B)$ - the subtree rooted at $B$. Furthermore $depth(B)$ denotes the depth of a block $B$ in the block tree.

\item The function $s()$ is defined to be the choice function that selects which block a new block should build off of. Formally, it maps a block tree $T = (V_T, E_T)$ onto a block $B \in V_T$. It should be noted that it is possible for different nodes to have different views of the block tree because of network propagation delays when announcing new blocks.

\item The Bitcoin protocol originally defined $s()$ as selecting the block with the largest depth in the block tree. As such, $longest(t)$ defines the deepest leaf block in $tree(t)$ at time $t$.

\item The "main chain" is defined to be the path from the genesis block to the block that $s()$ selects to be the next block to build off of (originally in Bitcoin this was $longest(t)$). The time it takes for the main chain grows from length $n-1$ to $n$ is a random variable defined as $\tau_n$. The rate of block addition to the main chain is defined as $\beta = \frac{1}{E[\tau]}$ where $\tau = lim_{n\rightarrow\infty}\frac{1}{n}\sum_{i=1}^n\tau_n$. I think this is essentially saying that the rate the main chain grows is defined in terms of the expected value of the infinite series of random variable main chain growth times. It should be noted that $\lambda$ denotes the rate at which blocks are added (not necessarily to the main chain) to the block tree.

\item The maximum block size in kilobytes is represented as $b$, and it is assumed that newly created blocks are always the maximal size

\item The primary measure of Bitcoin scale is defined to be Transactions Per Second (TPS) the system adds to the main chain and is defined to be $TPS(\lambda, b) = \beta(\lambda, b) \times b \times K$ where $K$ is the average number of transactions per $KB$. Since $\beta$ measures block creation over time, $b$ measures kilobytes, and $K$ measures transactions transactions per kilobytes, we get the transactions per seconds.
\end{itemize}

\item The amount of throughput in the network determines how susceptible Bitcoin is to double-spend attacks. If an attackers compute power is is greater than the main chain block addition rate ($p\lambda_h > \beta$), then the attacker's chain will always win out in the end eventually. Conversely if $q < \frac{\beta}{\lambda_h}$, then the probability of the attacker winning decreases exponentially as the main chain grows in length. Therefore $\frac{\beta}{\lambda_h}$ is defined to be the security threshold of the system.

\item Transaction throughput is controlled mostly by block creation rate ($\lambda$) and block size ($b$).

\item Some naive attempts to increase throughput pose increased security risk to Bitcoin:
\begin{itemize}
\item \textbf{Larger Blocks:} Increased block size cause more forks because of longer block propagation delays in the system. Propagation time has been shown to grow linearly with block size

\item \textbf{Accelerated Block Creation:} If blocks are created faster by the honest network (larger $\lambda_h$), then there is a loss of efficiency because nodes will be more likely to contribute to an outdated fork as opposed to the main branch (some nodes will not be fully synced on the most up to date state of the tree). Conversely attackers will also have increased block creation ($b\lambda_h$) but will not experience any of the same efficiency loss because of the assumptions that attackers have full, efficient control over their resources.
\end{itemize}

\item The main issue here seems to be that trying to scale transaction throughput using the longest chain rule leads to more forks because nodes cannot reach consensus as quickly because of increased block propagation delays. As a result this becomes an attack vector bad actors can exploit with secret mining.

\item Greediest Heaviest Observed Subtree (GHOST):
\begin{itemize}
\item GHOST allows for cryptocurrency protocol designers to set high block creation rates and block sizes.

\item Originally, the main chain in the Bitcoin protocol is chosen to be the longest chain. At a high level, GHOST changes the weight of a chain so that it takes into account blocks that "hang" off the main chain but are not a part of the main chain (essentially forks that hang off the main chain contribute to the weight of the main chain).

\item Formally, GHOST redefines the parent selection function ($s()$). For a block $B$ in a tree $T$, let $subtree(B)$ be a subtree of $T$ rooted at $B$ and let $Children_T(B)$ be the set of blocks directly referencing $B$ as their parent block. Defined $GHOST(T)$ to be a parent selection function that follows this algorithm with input block tree $T$:
\begin{enumerate}
\item $current\_node := Genesis Block$
\item if $Children_T(current\_node) = \emptyset$ then $return(current\_node)$
\item else $current\_node := node from Children_T(current\_node) whose subtree is the largest size$
\item goto step 2
\end{enumerate}
It should be noted that the size of a subtree is directly correlated with the hardest combined proof-of-work
\end{itemize}
\item Properties of GHOST:
\begin{itemize}
\item Every block is either fully abandoned or fully adopted into the main chain

\item The probability that a block is initially on the main chain and then becomes off the main chain ($50\%$ attack) can be made arbitrarily small given a sufficiently long waiting time $\tau$ after the blocks' creation. This means the security threshold ($\frac{\beta}{\lambda_h}$) can be made to equal $1$, meaning that the attackers' block creation rate must equal that of the honest network.

\item The rate of growth of the main chain ($\beta$) using GHOST is slightly slower than using the longest chain rule. However unlike the longest chain rule, this slowdown in growth does not effect the security of the GHOST rule.

\item The authors attempt to measure the efficiency of the GHOST rule and provide a framework for protocol designers to set security parameters (e.g. block creation rate) using simulations of overlay networks since the entire network for a cryptocurrency protocol cannot be known beforehand. I mostly skimmed this section, but its interesting to note in case I would ever need to do something similar
\end{itemize}

\item Although the security threshold ($\frac{\beta}{\lambda_h}$) of GHOST is always $1$, if the throughput of the grows limitlessly then bandwidth becomes an efficiency bottleneck

\item Their simulations show that the GHOST rule and longest chain rule both can scale TPS at approximately the same rate as block creation increases (however longest chain suffers from decreased security threshold while GHOST remains at $1$)

\item The acceptance policy (time a merchant is willing to wait before considering a transaction as confirmed) can be modeled as a function $n(t, r, q)$ where $r$ is the risk the vendor is willing to take and $q$ is the upper bound on the attackers' compute power and $t$ is the time elapsed since the transaction was broadcast to the network. Vendors wait until $n \ge n(t, r, q)$ blocks have piled up on the transaction block before considering the transaction as being confirmed.

\item blocks using GHOST get confirmations from all other blocks in its subtree. 

\item Changes in the implementation of GHOST:
\begin{itemize}
\item Only send the headers of off-main-chain blocks since all nodes must know of these blocks.

\item Deployment of GHOST rule can be gradual since it is mostly compatible with the longest chain rule at low block creation rates

\item Only award coins for mining to block creators that mine blocks that end up on the main chain. Block creators that create blocks that end up in the same subroot should not be awarded coins

\item To avoid attackers adding blocks to off-main-chain branches long in the past (where the difficulty target might be lower), it is suggested to using a checkpointing system that prevents addition of new blocks prior to some block in the tree
\end{itemize}

\item A final note: I don't believe that at this time Bitcoin uses the GHOST protocol. Ethereum uses a modified rule which awards coins to block creators that contributed to blocks that hang off the main chain. Also there seems to be some criticism stating the GHOST would centralize mining because well connected nodes benefit or something or other, I dunno it kinda seemed like the jury was out plus these were just comments on reddit and y combinator so who knows if the commenters actually knew what they were talking about
\end{itemize}

\section{Consensus}

\subsection{Paxos Made Simple}
Simple description of the Paxos algorithm for distributed consensus

\begin{itemize}
\item Consensus Algorithm
\begin{itemize}
\item Problem Statement
\begin{itemize}
\item Have a collection of distributed processes which can propose values

\item Want a consensus algorithm that ensures that only a single value out of the proposed values is chosen such that 1. Only a value that has been proposed can be chosen 2. Only a single value is chosen 3. A process never learns that a value is chosen unless has actually been chosen

\item There are three types of roles in the algorithm: $proposers$, $acceptors$, and $learners$.

\item Processes communicate with one another by passing messages and we use an asynchronous Non-Byzantine model: 1. Processes operate at arbitrary speeds, can fail by stopping and restarting, has stable storage to remember information (if not then solution is impossible since all processes could fail after value selection) 2. There are no bounds for message delivery delays and can be duplicated and lost but not corrupted.

\end{itemize}

\item Choosing a Value
\begin{itemize}
\item A simple solution is to have a single $acceptor$ which accepts the first value it receives. \textbf{Problem:} Single $acceptor$ becomes single point of failure

\item Another way it to have multiple $acceptors$ which $accept$ different proposed values from processes. A value is chosen when a majority of $acceptors$ have $accepted$ it (this works because any two majorities have at least one $acceptor$ process in common)

\item \textbf{P1. An acceptor must accept the first proposal it receives}

\item In order to satisfy \textbf{P1}, we must allow $acceptors$ to be able to $accept$ multiple values. 

\item A proposal now consists of a unique proposal number (to uniquely identify proposals) and a value that is being proposed.

\item \textbf{P2. If a proposal with value $v$ is chosen, then every higher-numbered proposal that is chosen has value $v$.}

\item \textbf{P2A. If a proposal with value $v$ is chosen, then every higher-numbered proposal $accepted$ by an $acceptor$ has value v}. We must still satisfy \textbf{P1}, so we must strengthen \textbf{P2A} to \textbf{P2B}

\item \textbf{P2B. If a proposal with value $v$ is chosen, then every higher-numbered proposal that is proposed by a proposer has value $v$.}

\item Note that since $acceptors$ accept proposals which are proposed by proposers, we have $\textbf{P2B} \rightarrow \textbf{P2A} \rightarrow \textbf{P2}$

\end{itemize}
\end{itemize}
\end{itemize}

\subsection{The Part-Time Parliament}

Describes the Paxos algorithm for reaching consensus in a distributed system

\begin{itemize}
\item Paxons' Parliament Description
\begin{itemize}
\item Primary task was to determine the law of the land

\item Each member kept track of a numbered ledger to record the laws that were passed by the parliament

\item First requirement was that member's ledgers were consistent (i.e. no two ledgers could contain conflicting information). If member $A$ has decree $X$ written at position $N$ then member $B$ cannot have decree $Y$ written at position $N$ in her ledger (she can leave position $N$ blank if she has not yet learned of decree $X$).

\item Note that fulling the consistency condition can be made trivial (all members leave their ledger blank), there must also be a progress condition - If a majority of the members remained in the Parliament Chamber without anyone entering or leaving for a sufficiently long period of time, then any decree proposed by a member would be passed and all passed decrees would appear in the ledger of every member
\end{itemize}
\end{itemize}

\section{Facebook}

\subsection{SVE: Distributed Video Processing at Facebook Scale}

Describes the evolution and architecture of Streaming Video Engine (SVE) which handles video uploads at Facebook scale.

\begin{itemize}
\item 3 key requirements of video processing at Facebook scale: low-latency, flexible API for application developers, and fault tolerance and resiliency. 

\item The video encoding engine validates uploaded videos and re-encodes the video into many bitrates and file formats in order to support a wide array of clients and stream video at the best quality based on network conditions.

\item Facebook orginally had a system dubbed Monolithic Encoding Script (MES) for uploading and processing videos, but it did not scale well.

\item SVE adds parallelism in 3 ways that MES did not: it parallelizes video validation and processing, it chunks videos into smaller pieces and parallelizes processing of the chunks over a compute cluster, and it parallelizes video processing with video storage + replication.

\item SVE's programming model has developers writing tasks that work on a stream-of-tracks abstraction that forms a Directed Acyclic Graph (DAG). The stream-of-tracks abstraction breaks a video down into streams (video, audio, metadata, etc.), and the DAG model allows tasks to be easily composed and parallelized.

\item 6 steps in the full video pipeline: record, upload, process, store, share, stream.

\item Users upload video to Facebook's front end server which then forwards it to SVE for processing.

\item The video is then validated which includes repairing the video (syncing audio + frames or fixing metadata) and validation (blocking malware and DOS attacks).

\item The video is next re-encoded which is the most computationally demanding step.

\item Once encoded, the video is stored in the Binary Large Object (BLOB) storage system (seems to be Facebook's in-house datastore).

\item When streaming, video clients download video chunks at the best bitrate it can sustain from Facebook's CDN.

\item This paper focuses on the pre-sharing + streaming phase of the video pipeline.

\item Monolithic Encoding Script:
\begin{itemize}
\item Had a simple architecture: Client uploads a file to the front end server as a single opaque binary blob. Once upload is completed, the front end server forwards the file directly to storage where it is picked up be a processing server which runs through the processing tasks sequentially with one encoding server handling a single video and then restored.

\item Worked but was not very flexible when adding new tasks or video applications. Hard to add monitoring

\item Not very reliable and crumbled when experiencing overload.
\end{itemize}

\item Existing batch processing frameworks such as MapReduce, Dryad, and Spark all assume that the data to be processed already exists (has been uploaded). This incurs high latency

\item Existing stream processing framework such as Storm, Spark Streaming, and Streamscope allow for overlapping of upload and processing but are designed to process continuous queries as opposed to discrete events.

\item Steaming Video Engine Overview
\begin{itemize}
\item Each server type is replicated many times, but only one server from each type handles a video.

\item 4 fundamental changes from MES architecture:

\begin{enumerate}
\item The uploading client tries to break the video up into segments (called group of pictures or GOP) to upload. Each GOP is processed and encoded independently. This segmenting allows processing to happen earlier since the front end server doesn't need to wait for the entire video to be uploaded to start forwarding it to the processing servers

\item The front end server forwards video chunks to a preprocessor server instead of directly to storage

\item The MES server has been replaced with the preprocessor, scheduler, and workers of SVE.

\item SVE exposes pipeline
construction in a DAG interface that allows application developers
to quickly iterate on the pipeline logic while providing
advanced options for performance tunings
\end{enumerate}

\item The Preprocessor does lightweight processing (which includes validation and breaking the video up into GOP segments if an old upload client does not support this), initiates heavyweight processing, and acts as a write-through cache since it writes video segments to main memory as well as passing them off to persistent storage for redundancy. Writing the segments to storage is done concurrently with heavyweight processing, so having the Preprocessor cache video segments helps move the Storage (and its slow use of disk) off of the critical path.

\item Video encoding is heavyweight since it operates at the pixel level and is done on a bunch of worker machines which are orchestrated by the Scheduler. The tasks to be run are determined by the DAG of tasks for the application that the video is being processed for.

\item The Scheduler receives the DAG job from the Preprocessor as well as notifications for when video segments have been preprocessed and are available. The scheduler pushes tasks to a cluster of workers. Each cluster has a high and low priority queue of remaining tasks they can pull from. The priority of a task is set by the application programmer. Workers will always pull from the high priority queue over the low priority queue. SVE shards video encoding by video ID. This means that a DAG job might be handled by many Schedulers during the course of its runtime due to capacity and load balancing demands; however, at any given time a DAG job is being processed only by a single Scheduler. 

\item Workers process tasks in the DAG job for a video in parallel. They receive tasks from the Scheduler, pull the input data to complete the task either from the Preprocessor's memory cache or from Intermediate Storage, and then write the task output either to Intermediate Storage (for further processing) or to persistent Storage.

\item Intermediate Storage is used in a variety of ways. The type of storage being used depends on the type of data being written. Application metadata is stored in a multi-tiered storage system that durably stores the data and makes it available through an in-memory cache because it is read much more frequently than written. The in-memory processing context of SVE is written to durable storage without caching since it is written many times and read only once. Video and audio data are written to a BLOB store and automatically freed after a few days. Facebook prefers having the storage layer automatically let the data expire before freeing it because they believe this leads to a simpler, more correct design.
\end{itemize}

\item Uploads are normally bottlenecked by the speed + upload bandwidth of the client. The solutions to speed this up is to either upload less data or parallelize uploading with encoding/processing. Another option is to process the video client side, this is done in SVE when the video is large and will effect pre-sharing latency and the client has the right specs to do this. Client-side processing of the video comes with tradeoffs because you're using the resources of the clinet (battery)

\item Segment size of video chunks when uploading is a tradeoff between compression and parallelization across segments. A larger segment size provides for better compression, and a smaller segment size provides for better parallelism

\item Syncing the video to durable storage can incur a significant amount of latency for some videos. SVE decreases latency by overlapping processing video segments with storing them. SVE also caches some segments in memory so that load times are quicker. For more on this design read the paper \textbf{Rethink the Sync}

\item DAG Execution System
\begin{itemize}
\item Programmers write sequential programs that act as tasks (vertices) in the DAG job. Edges of the graph represent dataflow and dependency.

\item A stream-of-tracks abstraction is used to specify the granularity at which tasks run. A task can run on one or more tracks (such as video or audio tracks) or it can run for all segments within a track.

\item Each DAG job is dynamically generate per-video by the Preprocessor. It probes the video and runs DAG generation code based on its findings (bitrate, etc.)

\item Once the DAG is created, the Preprocessor forwards it to the Scheduler which is in charge of dispatching tasks among workers and monitoring the progress of tasks and the job. The Preprocessor will alert the Scheduler when new video segments are available for processing.

\item Tasks can be organized into "task-grouops" which are collections of tasks that will all be run on one worker. Each task is by default in their own task-group. task-groups allow application developers to amortize scheduling overhead. Tasks can also be marked as "latency-sensitive" which means a user is waiting for the task to finish. All parent tasks of a latency-sensitive task are also marked as latency-sensitive. 

\end{itemize}

\item Fault Tolerance
\begin{itemize}
\item Workers that detect task failures (exception or failure exit code) will retry the task a few times and then propagate the failure to the Scheduler which will also retry scheduling the task among different workers. If the task continues to fail after retry, the task is marked as failed and the DAG job will fail if the task was critical.

\item User client's devices will anticipate intermittent uploads

\item The fron-end server will replicate state externally

\item The Preprocessor will replicate state externally

\item The Scheduler will synchronously replicate state externally

\item The Worker will replicate in time

\item Tasks will retry

\item The Storage will replicate over multiple disks

\end{itemize}


\item Overload
\begin{itemize}
\item Organic overload occurs when some event causes video upload and processing demand to spike beyond normal weekly spikes. New Years Eve is a good example.

\item Facebook's loadtesting framework purposefully overloads SVE to make sure that overload diagnostics are working as intended

\item Overload can also be induced by bugs in SVE (i.e. a memory leak causes the pipeline to slow or stall)

\item SVE first reacts to overload by delaying the scheduling of latency-insensitive tasks

\item SVE will then react to overload by shifting load to another regional SVE (this is done manually be an engineer)

\item Finally SVE if the other measures do not mitigate overload, SVE will delay processing of newly uploaded videos until older videos have finished processing.
\end{itemize}


\end{itemize}


\section{Google}

\subsection{The Chubby lock service for loosely-coupled distributed systems}

Chubby is a lock service that allows distributed processes to synchronize their actions and reach agreement on info about their environment

\begin{itemize}
\item Goals of Chubby are reliability, and availability, easy to understand semantics

\item Uses Paxos to solve distributed consensus and leader election. Augments with clocks to achieve liveness

\item Provides course-grained locking - locks are expected to be held by the primary for a long duration

\item System Structure
\begin{itemize}
\item Two main components: server and a library used by clients

\item A Chubby cell consists of 5 replicas that choose a master via a consensus protocol. The master is chosen by a majority vote between the replicas and is guaranteed by to be the only master for a short interval known as the master lease time.

\item Each replica maintains a simple database. Only the master coordinates reads and writes to this database. The replicas only copy updates from the master via the consensus protocol

\item A client trying to connect to the Chubby service first must locate the master of the Chubby cell. It will send a master identification request to the replicas listed in the DNS. Replicas respond to this query by returning the identity of the master. The client then directs all future queries to the master unless the master becomes unresponsive or indicates it is no longer the master

\item The master propagates write requests to all replicas and returns only once a majority of replicas have accepted the write request. The master services read requests itself since it is guaranteed to be the only master (note this is only safe if the master lease has not expired)

\item If a master fails, the replicas will select a new master once the master lease is up. This normally takes only a few seconds

\item If a replica fails for a long enough period of time, a monitoring recovery system will notice and start a new replica on a fresh machine. It will also update the DNS record and replace the IP address of the failed replica with that of the new one. The master periodically polls the DNS record and will learn of the new replica. The master will update the replica list in the database it maintains which will then get propagated to the other replicas via the consensus protocol. The new replica will eventually get a copy of this database via the consensus protocol but starts itself using backups of the database and information from the other replicas. The new replica is allowed to vote in the master election once it has responded to a write request from the current master
\end{itemize}

\item Files, directories, and handles
\begin{itemize}
\item Exposes a UNIX like filesystem containing a tree of files and directories

\item An example path is $/ls/foo/wombat/pouch$. $ls$ (lock service) is the prefix for all Chubby names. $foo$ is the name of a Chubby cell that resolves to a Chubby server via DNS. $local$ is the name of a special Chubby cell that represents the Chubby cell local to the client (normally in the same building)

\item In order to allow different Chubby masters to serve files in different directories, there isn't an operation to move a file between directories, directory modification time is not recorded, and file access is controlled by permissions on the file itself

\item The system does not expose file access times in order to make it easier to cache file metadata

\item Files and directories are called nodes (this is poor naming in my opinion)

\item No symbolic or hard links for a node within a Chubby cell

\item Nodes may be permanent or ephemeral. Nodes may be deleted and ephemeral nodes are also deleted if no client has them open. 

\item Ephemeral files are temporary and used as a means for indicating that a client is still alive to other clients

\item Access to nodes is maintained by ACL files which reside in a well-kown part of the Chubby cell's namespace. Each node's metadata points to 3 different ACL files (readfile, writefil, ACL modification file).

\item ACL files contain names of users that are permitted to access the node in some way (read, write, modify ACL permissions)

\item Nodes' metadata also includes other fields: an instance number that is greater than the instance number of any previous node with the same name, a content generation number that increases when the files contents are written (not for directories), a lock generation number which increases when a node's lock transitions from $free$ to $held$, and and ACL generation number which increases when the node's ACL names are modified

\item A 64-bit file content checksum is also exposed so clients can tell if a file differs

\item Clients open nodes to obtain handles which are like file descriptors. Handles contain: check digits which prevent clients from forging handles (ACL permissions need only be enforced at handle creation), a sequence number which signals to the Chubby master if the handle was created by itself or a previous master, and mode information which is provided at handle creation which allows a newly restarted master to recreate its state if an old handle is presented to it
\end{itemize}

\item Locks and sequencers
\begin{itemize}
\item Each Chubby file or directory can act as a reader/writer lock

\item One client may exclusively hold the lock in write mode while many readers can share the lock in read mode

\item Locks are advisory, meaning clients need not hold any locks to interact with files. However; attempts to hold the same lock may conflict

\item Acquiring a lock in either mode requires write permission so that an unprivileged reader cannot stall a writer

\item A problem that can occur in a distributed environment that uses is locking is the following situation: Process $A$ acquires lock $L$ and sends request $R$ but then fail after sending $R$. Process $B$ then acquires $L$ and sends message $R2$ which arrives before $R$ because of delays. $R$ will now act on data without the protection of $L$

\item Sequence numbers are introduced to all messages that make use of a lock in order to guard against message delays

\item A lock holder may request a $sequencer$ - an opaque byte-string that describes the state of the lock after acquisition. The $sequencer$ contains the name of the lock, lock generation number, and mode of the lock. The client passes the $sequencer$ to other clients if it expects operations to be protected by the lock. The recipient clients must validate that the $sequencer$ is valid (this can be done by checking its Chubby cache). 

\item Chubby provides an (imperfect) additional guard against message delays. If a lock becomes free because of client failure, Chubby does not permit other clients from acquiring the lock for a configurable duration known as the $lock delay$
\end{itemize}

\item Events
\begin{itemize}
\item Clients may subscribe to events when they create a handle: file contents modified (normally used to monitor the change in a service advertised via a file), child node add/removed modified, Chubby master fail over, a handle + lock has become invalid, lock acquired, conflicting lock requests

\item 
\end{itemize}

\item API
\begin{itemize}
\item $Open()$ takes a file or directory name relative to an existing directory handle (a valid handle to the '/' directory is always maintained by the client library) and returns a handle to that node. Client additionally specifies the mode to open the handle in, events to listen for, the $lock-delay$ time, and initial file contents + ACL list if the node to be opened will be newly created

\item $Close()$ closes an open handle and never fails

\item $Poison()$ invalidates any outstanding and future requests made using a handle without closing the handle

\item $GetContentsAndStat()$ returns the full content + metadata for a file. Files are read atomically

\item $GetStat()$ returns just the metadata and $ReadDir()$ returns children + metadata for a directory handle

\item $SetContents()$ writes data to the file handle. Files are written atomically. Client may optionally add a generation number. The contents will then only be written if the supplied generation number is more current. $SetACL()$ does the analogous operation for ACL names

\item $Delete()$ deletes the node if it has no children

\item $Acquire()$, $TryAcquire()$, and $Release()$ all deal with locking

\item $GetSequencer()$ obtains a sequencer describing any lock held by this handle

\item $SetSequencer()$ associates a sequencer with a handle. All subsequent operations on the handle will fail if the sequencer becomes invalid.

\item $CheckSequencer()$ checks if a sequencer is valid

\item Handles are only associated with a specific $instance$ of a node rather than the node name itself. So operations on a handle for a node will fail if the node is deleted even if it is subsequently recreated

\item ACL permissions are always enforced during $Open()$ calls and may be enforced on subsequent calls

\item To make calls asynchronous, all operations may take a callback

\item Master election can be performed using Chubby's API as follows: all potential primaries for a service open the lock file and attempt to acquire the lock. One succeeds and becomes the leader. The leader writes its address in the lock file using $SetContents()$, so it can be discovered by the replicas and clients of the service. Clients + Replicas learn of the primary's address using $GetContentsAndStat()$ (probably in response to a file modification event). Ideally, the primary creates a sequencer with $GetSequencer()$ which it passes to all servers it communicates with. Servers should verify the primary is still the current master by verifying the sequencer using $CheckSequencer()$.
\end{itemize}

\item Caching
\begin{itemize}
\item Since most Chubby traffic are read requests, it is desirable to employ caching in order to reduce the amount of read traffic

\item Chubby clients cache file data and node metadata in a consistent, write-through cache held in memory. The Chubby cell master keeps track of what each client $may$ be caching and sends cache invalidations to clients in order to keep their caches consistent

\item When a write request for file data or node metadata comes into the Chubby master, it blocks the operation and sends (via KeepAlive RPC responses) cache invalidation events to all clients which may have cached the data to be changed. Once the Chubby master has received acknowledgement of the cache invalidation event from all of the clients it contacted (either through another KeepAlive RPC request or the expiry of the client's cache lease), it unblocks the write operation. Since writes are only a small portion of Chubby's traffic, its okay for them to be slower

\item While the Chubby master is waiting for clients to respond to cache invalidation acknowledgments for a node, it treats the node as uncachable, meaning clients sending read requests for the node will not cache the node. However, this means that reads requests never have to block

\item Clients acknowledge a cache invalidation event by removing the cached data and then making a KeepAlive RPC request

\item Client caching is simple - clients can only cache and invalidate objects. There is no updating cached objects. If updates were allowed, clients that accessed a file once and then never again could potentially get a large number of unnecessary cache update events 

\item Clients can also cache open handles. If a client calls $Open()$ on a node that it had previously opened, only the first RPC call will make it to the Chubby master. Clients may also cache locks
\end{itemize}

\item Sessions and KeepAlives
\begin{itemize}
\item Sessions between clients and Chubby cells are maintained via periodic KeepAlive RPCs. Sessions guarantee that all of the client's cached data, locks, and handles remain valid while the session is valid.

\item Clients request sessions by contacting the Chubby master. Sessions are ended either explicitly or when the client has idled for too long

\item Sessions have a lease time during which the Chubby master may not terminate the session. The master is free to extend this lease time

\item Session leases are extend in three circumstances: Upon session creation, when master failover occurs, and when a master responds to a KeepAlive RPC.

\item When a Chubby master receives a KeepAlive request, it blocks the call until right before the client's session lease is about to expire. It then returns a response to the client with the new session lease time. The client then immediately sends another KeepAlive RPC. 

\item KeepAlive RPCs are also used by the master to send events and cache invalidations to the client. A master will return a KeepAlive RPC response early if such an event has occurred. This simplifies the Chubby protocol and allows client - master communication to work through firewalls which only allow establishing connections one way

\item The client maintains a local session lease timeout which is a conservative approximation of the master's lease timeout. This means the master's clock must not advance faster than the client's clock by no more than a constant factor

\item When a client's local lease time expires, it can't be sure if the master has terminated its sessions, so it enters into a $session jeopardy$ mode. The client empties and disables its cache and tries to establish another KeepAlive RPC with the master with a timeout. If the client and master exchange a successful KeepAlive RPC, the client enables its cache again; otherwise, it assumes its session is terminated. This is so that the client does not block indefinitely waiting for RPCs to be returned from the Chubby master

\item The Chubby library will notify the application if its session goes into jeopardy with an event. The library will also send either a session $safe$ or $expired$ event once the session either recovers or is confirmed to be ended.
\end{itemize}

\item Fail-overs
\begin{itemize}
\item When a Chubby master fails or loses mastership, it discards all of its in-memory state about sessions, handles, and locks. The timer for sessions leases also stops during this time since it only runs at the current master.

\item When a master failure or change occurs, clients session can potentially go into $jeopardy$ depending on how long the mastership change takes

\item When a new Chubby master is elected and a client has contacted it, the new Chubby master must recreate the in-memory state of the old master. It does this by reading data on disc (obtained through the database replication protocol), by obtaining state from clients, and by conservative assumptions. The database records each session, held lock, and ephemeral files

\item New elected masters:
\begin{enumerate}
\item Picks a new $epoch number$ which must be sent by clients on every call to the master. The master will reject client requests using an older $epoch number$

\item The master may respond to new master location requests but does not when first processing a session

\item It builds the in-memory data the old master had for sessions and locks from the database. It also extends session lease

\item The master now lets clients perform KeepAlive requests but no other session-related operations

\item It emits a mastership change event to all sessions. This lets clients know to flush their caches and warn their applications of potentially lost messages

\item The master waits for all sessions to acknowledge the mastership change or let their session lease expire

\item The master allows all operations to proceed

\item If a client uses a handle created from a previous master (based on the sequence number of the handle), the master recreates the in-memory representation of the handle and accepts the requests. If a recreated handle is closed, then the master notes this so that it cannot be recreated in $this$ (but possibly future) master epoch

\item After a short interval, the master deletes all ephemeral files that have no open file handles. Therefore, clients should promptly refresh handles on ephemeral files after a mastership change
\end{enumerate}
\end{itemize}

\item Database Implementation
\begin{itemize}
\item Chubby initially used the replicated version of Berkley's DB

\item Google however chose to write their own replicated database using write-ahead logging and snapshotting similar Birell $et all$ 
\end{itemize}

\item Backup
\begin{itemize}
\item Every so often the master of a Chubby cell writes a snapshot of its data to a GFS file server in another building. Different buildings provide data redundancy in the case of building damage as well as eliminating cyclic dependencies between GFS cells and Chubby cells (since a GFS cell might use the Chubby cell in its building for mastership election)

\item Backups allow newly replaced replicas to be bootstrapped without placing load on existing replicas in the cell
\end{itemize}

\item Mirroring
\begin{itemize}
\item Chubby allows mirroring of files between cells.

\item Mirroring is most commonly used to deploy configuration files around the globe

\item A special Chubby cell called $global$ has a subtree called $/ls/global/master$ which is mirrored at every Chubby cell under the subtree $/ls/\$cell/slave$. Each member of the $global$ cell is spread out across the globe so that any client can contact this cell

\item The $/ls/global/master$ subtree contains Chubby's own ACL info as well as various files with locations of Chubby cell's where certain services (such as BigTable) are advertised  
\end{itemize}

\item Mechanisms for Scaling
\begin{itemize}

\end{itemize}

\end{itemize}


\section {Paper Backlog}

\begin{itemize}
\item \textbf{Post-quantum RSA} by Daniel J. Bernstein, Nadia Heninger, Paul Lou, and Luke Valenta

\item \textbf{A Fistful of Bitcoins: Characterizing Payments Among Men with No Names} Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, Kirill Levchenko, Damon McCoy, Geoffrey M. Voelker, Stefan Savage
\end{itemize}


% \section{Introduction}

% Your introduction goes here! Some examples of commonly used commands and features are listed below, to help you get started. If you have a question, please use the help menu (``?'') on the top bar to search for help or ask us a question. 

% \section{Some examples to get started}

% \subsection{How to include Figures}

% First you have to upload the image file from your computer using the upload link the project menu. Then use the includegraphics command to include it in your document. Use the figure environment and the caption command to add a number and a caption to your figure. See the code for Figure \ref{fig:frog} in this section for an example.

% \begin{figure}
% \centering
% \includegraphics[width=0.3\textwidth]{frog.jpg}
% \caption{\label{fig:frog}This frog was uploaded via the project menu.}
% \end{figure}

% \subsection{How to add Comments}

% Comments can be added to your project by clicking on the comment icon in the toolbar above. % * <john.hammersley@gmail.com> 2016-07-03T09:54:16.211Z:
% %
% % Here's an example comment!
% %
% To reply to a comment, simply click the reply button in the lower right corner of the comment, and you can close them when you're done.

% Comments can also be added to the margins of the compiled PDF using the todo command\todo{Here's a comment in the margin!}, as shown in the example on the right. You can also add inline comments:

% \todo[inline, color=green!40]{This is an inline comment.}

% \subsection{How to add Tables}

% Use the table and tabular commands for basic tables --- see Table~\ref{tab:widgets}, for example. 

% \begin{table}
% \centering
% \begin{tabular}{l|r}
% Item & Quantity \\\hline
% Widgets & 42 \\
% Gadgets & 13
% \end{tabular}
% \caption{\label{tab:widgets}An example table.}
% \end{table}

% \subsection{How to write Mathematics}

% \LaTeX{} is great at typesetting mathematics. Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
% \[S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
%       = \frac{1}{n}\sum_{i}^{n} X_i\]
% denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


% \subsection{How to create Sections and Subsections}

% Use section and subsections to organize your document. Simply use the section and subsection buttons in the toolbar to create them, and we'll handle all the formatting and numbering automatically.

% \subsection{How to add Lists}

% You can make lists with automatic numbering \dots

% \begin{enumerate}
% \item Like this,
% \item and like this.
% \end{enumerate}
% \dots or bullet points \dots
% \begin{itemize}
% \item Like this,
% \item and like this.
% \end{itemize}

% \subsection{How to add Citations and a References List}

% You can upload a \verb|.bib| file containing your BibTeX entries, created with JabRef; or import your \href{https://www.overleaf.com/blog/184}{Mendeley}, CiteULike or Zotero library as a \verb|.bib| file. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|.

% You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

% We hope you find Overleaf useful, and please let us know if you have any feedback using the help menu above --- or use the contact form at \url{https://www.overleaf.com/contact}!

% \bibliographystyle{alpha}
% \bibliography{sample}

\end{document}